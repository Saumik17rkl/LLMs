import gradio as gr
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Set environment variable for Hugging Face token
os.environ["HF_TOKEN"] = "You_Hugging_face_api_key"

# Load model and tokenizer
model_name = "google/gemma-2-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    use_auth_token=True
)

def generate_response(prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
    response = model.generate(**input_ids, max_new_tokens=512)
    output = tokenizer.decode(response[0], skip_special_tokens=True)
    return output

# Define the Gradio interface
iface = gr.Interface(
    fn=generate_response,
    inputs=gr.Textbox(placeholder="Enter your prompt here..."),
    outputs="text",
    title="LLM Text Generator",
    description="Generate text using Google's Gemma-2-2b-it model."
)

# Launch the Gradio UI
if __name__ == "__main__":
    iface.launch()
